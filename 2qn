import cv2
import numpy as np

# Load the input video
video_path = "/Users/akashdoss/Downloads/in-y2mate.com - HAMMER Blade 2 Pro video sample 2 nonstabilized_1080p.mp4"
cap = cv2.VideoCapture(video_path)

# Get video properties
n_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = cap.get(cv2.CAP_PROP_FPS)

# Read the first frame
_, prev_frame = cap.read()
prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)

# Initialize transformation matrix to store the translation parameters (dx, dy)
transforms = np.zeros((n_frames - 1, 2), np.float32)

for i in range(n_frames - 1):
    # Read the next frame
    success, curr_frame = cap.read()
    if not success:
        break

    # Convert the current frame to grayscale
    curr_gray = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)

    # Detect feature points in previous frame
    prev_pts = cv2.goodFeaturesToTrack(prev_gray, maxCorners=200, qualityLevel=0.01, minDistance=30, blockSize=3)

    # Calculate optical flow (motion estimation)
    curr_pts, status, _ = cv2.calcOpticalFlowPyrLK(prev_gray, curr_gray, prev_pts, None)

    # Filter valid points
    if curr_pts is not None and prev_pts is not None and len(prev_pts) > 0 and len(curr_pts) > 0:
        # Reshape points for proper indexing
        prev_pts = prev_pts.reshape(-1, 2)
        curr_pts = curr_pts.reshape(-1, 2)

        # Filter out points where optical flow was unsuccessful
        idx = np.where(status == 1)[0]
        prev_pts = prev_pts[idx]
        curr_pts = curr_pts[idx]

        if len(prev_pts) > 0 and len(curr_pts) > 0:
            # Estimate translation (ignore rotation)
            dx = np.mean(curr_pts[:, 0] - prev_pts[:, 0])
            dy = np.mean(curr_pts[:, 1] - prev_pts[:, 1])

            # Store the translation transformations
            transforms[i] = [dx, dy]
        else:
            # No valid points, use no transformation
            transforms[i] = [0, 0]
    else:
        # No valid points, use no transformation
        transforms[i] = [0, 0]

    # Update previous frame
    prev_gray = curr_gray.copy()

# Compute the cumulative sum of translations (trajectory)
trajectory = np.cumsum(transforms, axis=0)

# Apply temporal smoothing (high-pass filtering)
smoothed_trajectory = cv2.blur(trajectory, (30, 1))  # Smoothing window size
difference = smoothed_trajectory - trajectory
transforms_smooth = transforms + difference  # Adjust transforms with smooth trajectory

# Reset the video to the first frame
cap.set(cv2.CAP_PROP_POS_FRAMES, 0)

for i in range(n_frames - 1):
    # Read the next frame
    success, frame = cap.read()
    if not success:
        break

    # Get the smoothed transformation
    dx = transforms_smooth[i, 0]
    dy = transforms_smooth[i, 1]

    # Create a translation matrix (ignore rotation)
    m = np.float32([[1, 0, dx], [0, 1, dy]])

    # Warp the current frame using only translation
    stabilized_frame = cv2.warpAffine(frame, m, (width, height))

    # Display the smoothed, stabilized frame
    cv2.imshow('Smoothed and Stabilized Frame', stabilized_frame)

    # Exit the video by pressing 'q'
    if cv2.waitKey(int(1000 / fps)) & 0xFF == ord('q'):
        break

# Release resources
cap.release()
cv2.destroyAllWindows()

print("Translation-based video stabilization complete.")
